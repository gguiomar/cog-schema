{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-20 15:56:31 __init__.py:190] Automatically detected platform cuda.\n",
      "Unsloth: Switching from Unsloth dynamic quant to normal quant since\n",
      "we do not yet support fast inference for unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\n",
      "==((====))==  Unsloth 2025.2.5: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.621 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit with actual GPU utilization = 25.51%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 23.62 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 4.26 GB. Also swap space = 5 GB.\n",
      "INFO 02-20 15:56:39 config.py:542] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-20 15:56:39 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit', speculative_config=None, tokenizer='unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "INFO 02-20 15:56:40 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-20 15:56:40 model_runner.py:1110] Starting to load model unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit...\n",
      "INFO 02-20 15:56:40 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W220 15:56:40.185148841 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 15:56:41 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2dc478bb12242ae847488ae46c09f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4fc6ce2b0140038ec5865f367b2deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 15:56:41 model_runner.py:1115] Loading model weights took 1.5365 GB\n",
      "INFO 02-20 15:56:41 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-20 15:56:43 worker.py:267] Memory profiling takes 1.42 seconds\n",
      "INFO 02-20 15:56:43 worker.py:267] the current vLLM instance can use total_gpu_memory (23.62GiB) x gpu_memory_utilization (0.26) = 6.03GiB\n",
      "INFO 02-20 15:56:43 worker.py:267] model weights take 1.54GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 2.14GiB; the rest of the memory reserved for KV Cache is 2.27GiB.\n",
      "INFO 02-20 15:56:43 executor_base.py:110] # CUDA blocks: 5311, # CPU blocks: 11702\n",
      "INFO 02-20 15:56:43 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 2.59x\n",
      "INFO 02-20 15:56:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:07<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 15:56:52 model_runner.py:1562] Graph capturing finished in 8 secs, took 0.54 GiB\n",
      "INFO 02-20 15:56:52 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 10.47 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from unsloth import FastLanguageModel\n",
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name=\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\",\n",
    "                max_seq_length=32768,\n",
    "                fast_inference=True\n",
    "            )\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prompt = tokenizer.encode(f\"<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>How many r's are in Strawberry?<ï½œAssistantï½œ>\", return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "#generated_text = model.generate(tokenized_prompt, max_new_tokens=100, do_sample=True, temperature=0.6, top_p=0.95)\n",
    "#print(tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ï½œUserï½œ>How many r's are in Strawberry?<ï½œAssistantï½œ><think>\n",
      "I need to determine how many times the letter 'r' appears in the word \"Strawberry.\"\n",
      "\n",
      "First, I'll write down the word: S T R A W B E R R Y.\n",
      "\n",
      "Next, I'll go through each letter and count the 'r's.\n",
      "\n",
      "The first letter is 'S' (no 'r'), the second is 'T' (no 'r'), the third is 'R' (count = 1), the fourth is 'A' (no 'r'), the fifth is 'W' (no 'r'), the sixth is 'B' (no 'r'), the seventh is 'E' (no 'r'), the eighth is 'R' (count = 2), the ninth is 'R' (count =3), and the tenth is 'Y' (no 'r').\n",
      "\n",
      "So, in total, there are three 'r's in the word \"Strawberry.\"\n",
      "Wait\n",
      "\n",
      "To determine how many times the letter 'r' appears in the word \"Strawberry,\" I will go through each letter and count each occurrence of 'r.'\n",
      "\n",
      "The word is: S, T, R, A, W, B, E, R, R, Y.\n",
      "\n",
      "Counting the 'r's:\n",
      "\n",
      "1. The first letter is 'S' â€“ no 'r.'\n",
      "2. The second letter is 'T' â€“ no 'r.'\n",
      "3. The third letter is 'R' â€“ that's one 'r.'\n",
      "4. The fourth letter is 'A' â€“ no 'r.'\n",
      "5. The fifth letter is 'W' â€“ no 'r.'\n",
      "6. The sixth letter is 'B' â€“ no 'r.'\n",
      "7. The seventh letter is 'E' â€“ no 'r.'\n",
      "8. The eighth letter is 'R' â€“ that's two 'r's.\n",
      "9. The ninth letter is 'R' â€“ that's three 'r's.\n",
      "10. The tenth letter is 'Y' â€“ no 'r.'\n",
      "\n",
      "So, there are three 'r's in total.\n",
      "\n",
      "However, upon closer inspection, I realize that the last two letters are 'R' and 'Y.' The 'R' is counted as the eighth letter, and the second 'R' is counted as the ninth letter. Therefore, there are three 'r's in the word \"Strawberry.\"\n",
      "Wait, no, that's not correct. Let's recount:\n",
      "\n",
      "1. S â€“ no 'r.'\n",
      "2. T â€“ no 'r.'\n",
      "3. R â€“ one 'r.'\n",
      "4. A â€“ no 'r.'\n",
      "5. W â€“ no 'r.'\n",
      "6. B â€“ no 'r.'\n",
      "7. E â€“ no 'r.'\n",
      "8. R â€“ two 'r's.\n",
      "9. R â€“ three 'r's.\n",
      "10. Y â€“ no 'r.'\n",
      "\n",
      "So, in total, there are three 'r's in the word \"Strawberry.\"\n",
      "Wait\n",
      "\n",
      "The word \"Strawberry\" has the letters: S, T, R, A, W, B, E, R, R, Y.\n",
      "\n",
      "Counting each 'r':\n",
      "\n",
      "- The third letter is 'R' â€“ that's one.\n",
      "- The eighth letter is 'R' â€“ that's two.\n",
      "- The ninth letter is 'R' â€“ that's three.\n",
      "\n",
      "So, there are three 'r's in total.\n",
      "\n",
      "Therefore, the correct answer is three 'r's.\n",
      "Wait\n",
      "\n",
      "The word \"Strawberry\" has the letters S, T, R, A, W, B, E, R, R, Y.\n",
      "\n",
      "Counting each 'r':\n",
      "\n",
      "1. S â€“ no 'r.'\n",
      "2. T â€“ no 'r.'\n",
      "3. R â€“ one 'r.'\n",
      "4. A â€“ no 'r.'\n",
      "5. W â€“ no 'r.'\n",
      "6. B â€“ no 'r.'\n",
      "7. E â€“ no 'r.'\n",
      "8. R â€“ two 'r's.\n",
      "9. R â€“ three 'r's.\n",
      "10. Y â€“ no 'r.'\n",
      "\n",
      "So, there are three 'r's in the word \"Strawberry.\"\n",
      "\n",
      "Wait\n",
      "\n",
      "The word \"Strawberry\" is spelled as S-T-R-A-W-B-E-R-R-Y.\n",
      "\n",
      "Counting each 'R':\n",
      "\n",
      "1. The third letter is 'R' â€“ that's one.\n",
      "2. The eighth letter is 'R' â€“ that's two.\n",
      "3. The ninth letter is 'R' â€“ that's three.\n",
      "\n",
      "So, there are three 'r's in the word \"Strawberry.\"\n",
      "</think>\n",
      "\n",
      "There are three 'r's in the word \"Strawberry.\"\n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Write Down the Word:**  \n",
      "   \"Strawberry\" is spelled as: S, T, R, A, W, B, E, R, R, Y.\n",
      "\n",
      "2. **Identify the Letter '\n"
     ]
    }
   ],
   "source": [
    "# Example parameters:\n",
    "min_thinking_tokens = 700  # Minimum tokens that must be produced between <think> and <\\think>\n",
    "max_output_tokens = 1000    # Total tokens we want in the final output\n",
    "max_batch_tokens = 20      # Generate tokens in small batches\n",
    "replacement_text = \"Wait\"  # Replacement text if end is premature\n",
    "\n",
    "# Encode the markers to get their token IDs.\n",
    "start_think_token = tokenizer.encode(\"<think>\", add_special_tokens=False)[0]\n",
    "end_think_token = tokenizer.encode(\"</think>\", add_special_tokens=False)[0]\n",
    "replacement_ids = tokenizer.encode(replacement_text, add_special_tokens=False)\n",
    "\n",
    "# Initialize flags and counters.\n",
    "is_thinking = False         # Are we currently in the thinking phase?\n",
    "thinking_token_count = 0    # Number of tokens produced during thinking (between <think> and <\\think>)\n",
    "\n",
    "# Start with the prompt (already on the modelâ€™s device).\n",
    "final_tokens = tokenized_prompt\n",
    "\n",
    "# Loop until the final sequence length reaches max_output_tokens.\n",
    "while final_tokens.shape[1] < max_output_tokens:\n",
    "    remaining = max_output_tokens - final_tokens.shape[1]\n",
    "    batch_new_tokens = min(max_batch_tokens, remaining)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=final_tokens,\n",
    "        max_new_tokens=batch_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    # Only the tokens generated in this batch (beyond what we already had)\n",
    "    new_tokens = outputs[:, final_tokens.shape[1]:]\n",
    "\n",
    "    for token in new_tokens[0]:\n",
    "        # If we've reached our overall length limit, stop.\n",
    "        if final_tokens.shape[1] >= max_output_tokens:\n",
    "            break\n",
    "\n",
    "        token_id = token.item()\n",
    "\n",
    "        # Handle start-of-thinking marker.\n",
    "        if token_id == start_think_token:\n",
    "            is_thinking = True\n",
    "            thinking_token_count = 0\n",
    "            final_tokens = torch.cat(\n",
    "                [final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Handle end-of-thinking marker.\n",
    "        if token_id == end_think_token:\n",
    "            if is_thinking:\n",
    "                if thinking_token_count < min_thinking_tokens:\n",
    "                    # Not enough thinking tokens generated yet:\n",
    "                    # Determine how many tokens we can inject without exceeding max_output_tokens.\n",
    "                    remaining_injection = max_output_tokens - final_tokens.shape[1]\n",
    "                    if len(replacement_ids) > remaining_injection:\n",
    "                        injection_ids = replacement_ids[:remaining_injection]\n",
    "                    else:\n",
    "                        injection_ids = replacement_ids\n",
    "                    final_tokens = torch.cat(\n",
    "                        [final_tokens, torch.tensor([injection_ids]).to(final_tokens.device)],\n",
    "                        dim=1\n",
    "                    )\n",
    "                    thinking_token_count += len(injection_ids)\n",
    "                    # Stay in thinking mode; do not exit it.\n",
    "                    continue\n",
    "                else:\n",
    "                    # Minimum thinking tokens reached; accept the end marker.\n",
    "                    final_tokens = torch.cat(\n",
    "                        [final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1\n",
    "                    )\n",
    "                    is_thinking = False\n",
    "                    continue\n",
    "            else:\n",
    "                # If not in thinking mode, just append the token.\n",
    "                final_tokens = torch.cat(\n",
    "                    [final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        # If we're in thinking mode, count the token.\n",
    "        if is_thinking:\n",
    "            thinking_token_count += 1\n",
    "\n",
    "        # Append the current token.\n",
    "        final_tokens = torch.cat(\n",
    "            [final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1\n",
    "        )\n",
    "        \n",
    "        # Stop early if the model outputs the EOS token.\n",
    "        if token_id == model.config.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Also break out of the loop if EOS was generated.\n",
    "    if final_tokens[0, -1].item() == model.config.eos_token_id:\n",
    "        break\n",
    "\n",
    "output_text = tokenizer.decode(final_tokens[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ï½œUserï½œ>How many r's are in Strawberry?<ï½œAssistantï½œ><think>\n",
      "Alright, so I'm trying to figure out how many times the letter 'r' appears in the word \"Strawberry.\" Let me start by writing down the word to make sure I have it correctly. \"Strawberry\" is\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Configuration and Parameters\n",
    "# ----------------------------\n",
    "max_output_tokens = 1024         # Overall maximum tokens in the output.\n",
    "batch_new_tokens = 50            # Number of tokens to generate in one batch.\n",
    "min_thinking_tokens = 500         # Minimum tokens that must be generated in thinking mode.\n",
    "start_think_token = 151648         # Hypothetical token ID for the start-of-thinking marker.\n",
    "end_think_token = 151649           # Hypothetical token ID for the end-of-thinking marker.\n",
    "replacement_text = \"Wait\"\n",
    "replacement_ids = tokenizer.encode(replacement_text, add_special_tokens=False)\n",
    "final_tokens = tokenized_prompt\n",
    "\n",
    "# ----------------------------\n",
    "# Generate New Tokens\n",
    "# ----------------------------\n",
    "# Generate a batch of new tokens from the model.\n",
    "outputs = model.generate(\n",
    "    final_tokens,\n",
    "    max_new_tokens=batch_new_tokens,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Extract only the new tokens (those beyond the already existing ones).\n",
    "new_tokens = outputs[:, final_tokens.shape[1]:]\n",
    "\n",
    "# ----------------------------\n",
    "# Custom Token-Processing Loop\n",
    "# ----------------------------\n",
    "# We'll use a phase variable to manage our three modes:\n",
    "# \"normal\"   : before any thinking marker is encountered.\n",
    "# \"thinking\" : after a start-of-thinking marker and before sufficient tokens have been generated.\n",
    "# \"answer\"   : after finishing the thinking phase, waiting for the answer's end marker.\n",
    "phase = \"normal\"\n",
    "thinking_token_count = 0\n",
    "answer_generated = False\n",
    "\n",
    "for token in new_tokens[0]:\n",
    "    # Stop processing if we've reached the overall token limit.\n",
    "    if final_tokens.shape[1] >= max_output_tokens:\n",
    "        break\n",
    "\n",
    "    token_id = token.item()\n",
    "\n",
    "    # If we see the start-of-thinking token, switch to thinking mode.\n",
    "    if token_id == start_think_token:\n",
    "        phase = \"thinking\"\n",
    "        thinking_token_count = 0\n",
    "        final_tokens = torch.cat([final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        continue\n",
    "\n",
    "    if phase == \"thinking\":\n",
    "        # Intercept the end-of-thinking token.\n",
    "        if token_id == end_think_token:\n",
    "            if thinking_token_count < min_thinking_tokens:\n",
    "                # Not enough thinking tokens have been produced; inject replacement tokens.\n",
    "                remaining_injection = max_output_tokens - final_tokens.shape[1]\n",
    "                if len(replacement_ids) > remaining_injection:\n",
    "                    injection_ids = replacement_ids[:remaining_injection]\n",
    "                else:\n",
    "                    injection_ids = replacement_ids\n",
    "                final_tokens = torch.cat(\n",
    "                    [final_tokens, torch.tensor([injection_ids]).to(final_tokens.device)],\n",
    "                    dim=1\n",
    "                )\n",
    "                thinking_token_count += len(injection_ids)\n",
    "                # Stay in thinking mode.\n",
    "                continue\n",
    "            else:\n",
    "                # Enough thinking tokens have been generated; switch to answer phase.\n",
    "                phase = \"answer\"\n",
    "                continue\n",
    "        else:\n",
    "            # In thinking mode, count and append tokens.\n",
    "            thinking_token_count += 1\n",
    "            final_tokens = torch.cat([final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            # Optionally, break if the model generates an EOS token.\n",
    "            if token_id == model.config.eos_token_id:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "    if phase == \"answer\":\n",
    "        # In answer phase, ignore tokens until an end-of-thinking token appears.\n",
    "        if token_id == end_think_token:\n",
    "            # Append the single answer token and exit.\n",
    "            final_tokens = torch.cat([final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            answer_generated = True\n",
    "            break\n",
    "        else:\n",
    "            # Skip any tokens until the end marker is generated.\n",
    "            continue\n",
    "\n",
    "    # Normal processing (when not in thinking or answer mode): append token as usual.\n",
    "    final_tokens = torch.cat([final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "    if token_id == model.config.eos_token_id:\n",
    "        break\n",
    "\n",
    "# ----------------------------\n",
    "# Decode and Output the Result\n",
    "# ----------------------------\n",
    "output_text = tokenizer.decode(final_tokens[0], skip_special_tokens=True)\n",
    "print(output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
