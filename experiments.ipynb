{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-19 14:52:53 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from unsloth import FastLanguageModel\n",
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching from Unsloth dynamic quant to normal quant since\n",
      "we do not yet support fast inference for unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\n",
      "==((====))==  Unsloth 2025.2.5: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.621 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit with actual GPU utilization = 22.33%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 23.62 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 160.\n",
      "Unsloth: vLLM's KV Cache can use up to 3.51 GB. Also swap space = 5 GB.\n",
      "INFO 02-19 14:53:50 config.py:542] This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-19 14:53:50 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit', speculative_config=None, tokenizer='unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":160}, use_cached_outputs=False, \n",
      "INFO 02-19 14:53:51 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-19 14:53:51 model_runner.py:1110] Starting to load model unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit...\n",
      "INFO 02-19 14:53:51 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W219 14:53:51.177053761 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 14:53:51 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbb859abe4847c0a48e42f9e589af3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767fde7dd13e4e6fa35283ab5d37a54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 14:53:52 model_runner.py:1115] Loading model weights took 1.5365 GB\n",
      "INFO 02-19 14:53:52 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-19 14:53:54 worker.py:267] Memory profiling takes 1.49 seconds\n",
      "INFO 02-19 14:53:54 worker.py:267] the current vLLM instance can use total_gpu_memory (23.62GiB) x gpu_memory_utilization (0.22) = 5.27GiB\n",
      "INFO 02-19 14:53:54 worker.py:267] model weights take 1.54GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 2.14GiB; the rest of the memory reserved for KV Cache is 1.50GiB.\n",
      "INFO 02-19 14:53:54 executor_base.py:110] # CUDA blocks: 3522, # CPU blocks: 11702\n",
      "INFO 02-19 14:53:54 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 1.72x\n",
      "INFO 02-19 14:53:55 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 23/23 [00:06<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 14:54:02 model_runner.py:1562] Graph capturing finished in 7 secs, took 0.46 GiB\n",
      "INFO 02-19 14:54:02 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 10.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name=\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\",\n",
    "                max_seq_length=32768,\n",
    "                fast_inference=True\n",
    "            )\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(f\"<｜begin▁of▁sentence｜><｜User｜>How many r's are in Strawberry?<｜Assistant｜>\", return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "#generated_text = model.generate(tokenized_prompt, max_new_tokens=100, do_sample=True, temperature=0.6, top_p=0.95)\n",
    "\n",
    "#print(tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜User｜>How many r's are in Strawberry?<｜Assistant｜><think>\n",
      "I need to determine how many times the letter 'r' appears in the word \"Strawberry.\"\n",
      "\n",
      "First, I'll write down the word: S T R A W B E R R Y.\n",
      "\n",
      "Next, I'll examine each letter individually to count the 'r's.\n",
      "\n",
      "The first 'r' is in the third position.\n",
      "The second 'r' is in the eighth position.\n",
      "The third 'r' is in the ninth position.\n",
      "\n",
      "Adding these up, there are three 'r's in total.\n",
      "</think>\n",
      "\n",
      "To determine how many times the letter **'r'** appears in the word \"Strawberry,\" let's break it down step by step:\n",
      "\n",
      "1. **Write down the word:**  \n",
      "   **Strawberry**\n",
      "\n",
      "2. **Identify each letter and count the 'r's:**  \n",
      "   - **S**  \n",
      "   - **T**  \n",
      "   - **R** (1st 'r')  \n",
      "   - **A**  \n",
      "   - **W**  \n",
      "   - **B**  \n",
      "   - **E**  \n",
      "   - **R** (2nd 'r')  \n",
      "   - **R** (3rd 'r')  \n",
      "   - **Y**\n",
      "\n",
      "3. **Count the total number of 'r's:**  \n",
      "   - **1st 'r'**  \n",
      "   - **2nd 'r'**  \n",
      "   - **3rd 'r'**\n",
      "\n",
      "   **Total:** **3**\n",
      "\n",
      "\\boxed{3}\n"
     ]
    }
   ],
   "source": [
    "# Example parameters:\n",
    "min_thinking_tokens = 700  # The minimum tokens that must occur between <think> and <\\think>\n",
    "max_output_tokens = 1000   # Total tokens we want to generate in this example\n",
    "max_batch_tokens = 20     # Generate tokens in small batches\n",
    "replacement_text = \" Let's continue thinking deeply... \"  # Replacement text if end is premature\n",
    "\n",
    "# Encode the markers to get their token IDs.\n",
    "start_think_token = tokenizer.encode(\"<think>\", add_special_tokens=False)[0]\n",
    "end_think_token = tokenizer.encode(\"<\\think>\", add_special_tokens=False)[0]\n",
    "replacement_ids = tokenizer.encode(replacement_text, add_special_tokens=False)\n",
    "\n",
    "# Initialize flags and counters.\n",
    "is_thinking = False         # Are we currently in the thinking phase?\n",
    "thinking_token_count = 0    # Number of tokens generated between <think> and <\\think>\n",
    "total_new_tokens = 0        # Overall new tokens generated\n",
    "\n",
    "# Start with your prompt (already on the model’s device).\n",
    "final_tokens = tokenized_prompt\n",
    "\n",
    "while total_new_tokens < max_output_tokens:\n",
    "    outputs = model.generate(\n",
    "        input_ids=final_tokens,\n",
    "        max_new_tokens=min(max_batch_tokens, max_output_tokens - total_new_tokens),\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "    # Extract only the new tokens (those beyond what we already had)\n",
    "    new_tokens = outputs.sequences[:, final_tokens.shape[1]:]\n",
    "    total_new_tokens += new_tokens.shape[1]\n",
    "\n",
    "    # Process the newly generated tokens one-by-one.\n",
    "    for token in new_tokens[0]:\n",
    "        token_id = token.item()\n",
    "        \n",
    "        # Check for the start thinking marker.\n",
    "        if token_id == start_think_token:\n",
    "            is_thinking = True\n",
    "            thinking_token_count = 0\n",
    "            final_tokens = torch.cat([final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            continue\n",
    "\n",
    "        # Check for the end thinking marker.\n",
    "        if token_id == end_think_token:\n",
    "            if is_thinking and thinking_token_count < min_thinking_tokens:\n",
    "                # Not enough tokens were produced during thinking.\n",
    "                # Substitute the end marker with our replacement text.\n",
    "                final_tokens = torch.cat(\n",
    "                    [final_tokens, torch.tensor([replacement_ids]).to(final_tokens.device)], dim=1\n",
    "                )\n",
    "                # Update our count by the number of tokens we injected.\n",
    "                thinking_token_count += len(replacement_ids)\n",
    "            else:\n",
    "                # Enough thinking tokens were generated; accept the end marker.\n",
    "                final_tokens = torch.cat([final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            is_thinking = False  # End the thinking phase.\n",
    "            continue\n",
    "\n",
    "        # If we’re in the thinking phase, update the token count.\n",
    "        if is_thinking:\n",
    "            thinking_token_count += 1\n",
    "\n",
    "        # Append the current token as usual.\n",
    "        final_tokens = torch.cat([final_tokens, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        \n",
    "        # Optionally, break early if EOS is generated.\n",
    "        if token_id == model.config.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Stop if we have reached the EOS token.\n",
    "    if final_tokens[0, -1].item() == model.config.eos_token_id:\n",
    "        break\n",
    "\n",
    "# Decode the full generated token sequence.\n",
    "output_text = tokenizer.decode(final_tokens[0], skip_special_tokens=True)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
