{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec87388-f751-4297-b96d-1b1e47cf8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!git clone https://github.com/gguiomar/cog-schema.git\n",
    "import os\n",
    "os.chdir(os.getcwd()+'/cog-schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5e289-1865-492b-b904-a032c806f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = \"workspace/cog-schema/agents/llama-centaur-adapter\"\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "os.chdir(destination_folder)\n",
    "!git clone https://huggingface.co/marcelbinz/Llama-3.1-Centaur-8B-adapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1937b3c-48b6-41b9-8109-411e5926bdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/cog-schema'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/workspace/cog-schema/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5484f267-8a65-4787-9d94-2da613d2419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using unsloth with GPU\n",
      "==((====))==  Unsloth 2025.1.6: Fast Llama patching. Transformers: 4.48.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.643 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from tasks.VSTtask import VSTtask\n",
    "from agents.LLMagent import LLMagent\n",
    "from manager.TaskManager import TaskManager\n",
    "\n",
    "model_name = \"marcelbinz/Llama-3.1-Centaur-8B-adapter\"\n",
    "pipe = LLMagent(\n",
    "    model_name=model_name, \n",
    "    device_map=\"cuda:0\", \n",
    "    max_seq_length=32768, \n",
    "    load_in_4bit=True, \n",
    "    use_unsloth=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "585bc9ad-d337-414d-9cde-0dd3af0e3edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:27<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "manager = TaskManager(n_simulations=100, nrounds=10, num_quadrants=2, num_queues=1, pipe=pipe, verbose=False)\n",
    "metrics = manager.run_simulations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b38bac0-1a2b-4032-bce6-cd1a53839ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e84df-2360-4b6c-bafc-12b1ea5efd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:14<00:00,  2.08it/s]\n",
      "100%|██████████| 30/30 [00:16<00:00,  1.79it/s]\n",
      "100%|██████████| 30/30 [00:19<00:00,  1.56it/s]\n",
      "100%|██████████| 30/30 [00:21<00:00,  1.40it/s]\n",
      "100%|██████████| 30/30 [00:24<00:00,  1.24it/s]\n",
      "100%|██████████| 30/30 [00:26<00:00,  1.13it/s]\n",
      " 67%|██████▋   | 20/30 [00:19<00:09,  1.00it/s]"
     ]
    }
   ],
   "source": [
    "metrics_l = []\n",
    "for n_rounds in np.arange(5,15,1):\n",
    "    manager = TaskManager(n_simulations=30, nrounds=n_rounds, num_quadrants=2, num_queues=1, pipe=pipe, verbose=False)\n",
    "    metrics_l.append(manager.run_simulations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d90f2d12-3986-4228-93dd-3b90c87b42e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_simulations': 100,\n",
       " 'success_rate': 0.51,\n",
       " 'quadrant_distribution': {'quadrant_1': {'times_chosen': 46,\n",
       "   'times_correct': 51},\n",
       "  'quadrant_2': {'times_chosen': 54, 'times_correct': 49}},\n",
       " 'timestamp': '20250128_094420'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b52d4f-6ff4-4789-9f8d-06f4f263c167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
